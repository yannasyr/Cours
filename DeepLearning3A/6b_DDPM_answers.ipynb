{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jc2XGY0IeBpb"
      },
      "source": [
        "# TP 6b - Generating images with Denoise Diffusion Probabilistic Models (DDPMs)\n",
        "\n",
        "DDPMs are generative models based on the idea of reversing a noising process. The idea is fairly simple: Given a dataset, make it more and more noisy with a deterministic process. Then, learn a model that can undo this process.\n",
        "\n",
        "DDPM-based models have recently drawn a lot of attention due to their high-quality samples. In this notebook, I re-implement the first and most fundamental paper to be familiar with when dealing with DDPMs: <i>Denoising Diffusion Probabilistic Models</i> (https://arxiv.org/pdf/2006.11239.pdf) by Ho et. al.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSf4ZzE-E84G"
      },
      "source": [
        "# Installs\n",
        "\n",
        "The only package we will need is the **einops** package. This will be used to create a nice GIF animation of our DDPM model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLfpRiqSE88Z"
      },
      "outputs": [],
      "source": [
        "!pip3 install --upgrade pip\n",
        "!pip3 install einops"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zRmDKhrHXvP"
      },
      "source": [
        "#Â Imports and Definitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4hfF9UGeANL"
      },
      "outputs": [],
      "source": [
        "# Import of libraries\n",
        "import random\n",
        "import imageio\n",
        "import numpy as np\n",
        "from argparse import ArgumentParser\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import einops\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torchvision.transforms import Compose, ToTensor, Lambda\n",
        "from torchvision.datasets.mnist import FashionMNIST"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from os import path\n",
        "from shutil import rmtree\n",
        "\n",
        "# add your own path. Where to save the models\n",
        "root_folder_Diff = '/content/drive/MyDrive/Lessons/Models/Diff/'\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "if path.exists(root_folder_Diff) :\n",
        "  rmtree(root_folder_Diff)\n",
        "os.makedirs(root_folder_Diff, exist_ok=True)\n",
        "store_path = path.join(root_folder_Diff, 'checkpoints')\n",
        "os.mkdir(store_path)"
      ],
      "metadata": {
        "id": "ia3D4NIdIcti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljv3LZ2v7YgQ"
      },
      "source": [
        "## Execution options\n",
        "\n",
        "Here's a few options you should set:\n",
        "\n",
        " - `no_train` specifies whether you want to skip the training loop and just use a pre-trained model. If you haven't trained a model already using this notebook, keep this as `False`. If you want to use a pre-trained model, load it in the colab filesystem.\n",
        "\n",
        "- `fashion` specifies whether you want to use the Fashion-MNIST dataset (`True`) or not and use the MNIST dataset instead (`False`).\n",
        "\n",
        "- `batch_size`, `n_epochs` and `lr` are your typical training hyper-parameters. Notice that `lr=0.001` is the hyper-parameter used by the authors.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FhyEQtM67YVJ"
      },
      "outputs": [],
      "source": [
        "no_train = False\n",
        "fashion = True\n",
        "batch_size = 128\n",
        "n_epochs = 20\n",
        "lr = 0.001"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2DgEVAoEd1E"
      },
      "source": [
        "# Utility functions\n",
        "\n",
        "Following are two utility functions: `show_images` allows to display images in a square-like pattern with a custom title, while `show_fist_batch` simply shows the images in the first batch of a DataLoader object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PnQA-V2TEd6d"
      },
      "outputs": [],
      "source": [
        "def show_images(images, title=\"\"):\n",
        "    \"\"\"Shows the provided images as sub-pictures in a square\"\"\"\n",
        "\n",
        "    # Converting images to CPU numpy arrays\n",
        "    if type(images) is torch.Tensor:\n",
        "        images = images.detach().cpu().numpy()\n",
        "\n",
        "    # Defining number of rows and columns\n",
        "    fig = plt.figure(figsize=(8, 8))\n",
        "    rows = int(len(images) ** (1 / 2))\n",
        "    cols = round(len(images) / rows)\n",
        "\n",
        "    # Populating figure with sub-plots\n",
        "    idx = 0\n",
        "    for r in range(rows):\n",
        "        for c in range(cols):\n",
        "            fig.add_subplot(rows, cols, idx + 1)\n",
        "\n",
        "            if idx < len(images):\n",
        "                plt.imshow(images[idx][0], cmap=\"gray\")\n",
        "                idx += 1\n",
        "    fig.suptitle(title, fontsize=30)\n",
        "\n",
        "    # Showing the figure\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkWnTOS0EitS"
      },
      "outputs": [],
      "source": [
        "def show_first_batch(loader):\n",
        "    for batch in loader:\n",
        "        show_images(batch[0], \"Images in the first batch\")\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GxdeBS4HZq9"
      },
      "source": [
        "## Loading data\n",
        "We will use the MNIST (or FashionMNIST) dataset and try to generate some new samples out of (nowhere, but starting from some) random gaussian noise. **NOTE**: It is important to normalize images in range `[-1,1]` and not `[0,1]` as one might usually do. This is because the DDPM network predicts normally distributed noises throughout the denoising process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJ5tnfbfeND8"
      },
      "outputs": [],
      "source": [
        "# Loading the data (converting each image into a tensor and normalizing between [-1, 1])\n",
        "transform = Compose([\n",
        "    ToTensor(),\n",
        "    Lambda(lambda x: (x - 0.5) * 2)]\n",
        ")\n",
        "ds_fn = FashionMNIST if fashion else MNIST\n",
        "dataset = ds_fn(\"./datasets\", download=True, train=True, transform=transform)\n",
        "loader = DataLoader(dataset, batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QOFIp14opS4K"
      },
      "outputs": [],
      "source": [
        "# Optionally, show a batch of regular images\n",
        "show_first_batch(loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YM7MlNGC76XS"
      },
      "source": [
        "## Getting device\n",
        "\n",
        "If you are running this codebook from Google Colab, make sure you are using a GPU runtime. For non-pro users, typically a *Tesla T4* GPU is provided."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXPKke_S76cb"
      },
      "outputs": [],
      "source": [
        "# Getting device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\\t\" + (f\"{torch.cuda.get_device_name(0)}\" if torch.cuda.is_available() else \"CPU\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RY_qhk428P17"
      },
      "source": [
        "# Defining the DDPM module\n",
        "\n",
        "We now proceed and define a DDPM PyTorch module. Since in principle the DDPM scheme is independent of the model architecture used in each denoising step, we define a high-level model that is constructed using a `network` parameter, as well as:\n",
        "\n",
        "- `n_steps`: number of diffusion steps $T$;\n",
        "- `min_beta`: value of the first $\\beta_t$ ($\\beta_1$);\n",
        "- `max_beta`: value of the last  $\\beta_t$ ($\\beta_T$);\n",
        "- `device`: device onto which the model is run;\n",
        "- `image_chw`: tuple contining dimensionality of images.\n",
        "\n",
        "The `forward` process of DDPMs benefits from a nice property: We don't actually need to slowly add noise step-by-step, but we can directly skip to whathever step $t$ we want using coefficients $\\alpha_bar$.\n",
        "\n",
        "For the `backward` method instead, we simply let the network do the job.\n",
        "\n",
        "Note that in this implementation, $t$ is assumed to be a `(N, 1)` tensor, where `N` is the number of images in tensor `x`. We thus support different time-steps for multiple images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Menl_lR8SCC"
      },
      "outputs": [],
      "source": [
        "# DDPM class\n",
        "class MyDDPM(nn.Module):\n",
        "    def __init__(self, network, n_steps=200, min_beta=10 ** -4, max_beta=0.02, device=None, image_chw=(1, 28, 28)):\n",
        "        super(MyDDPM, self).__init__()\n",
        "        self.n_steps = n_steps\n",
        "        self.device = device\n",
        "        self.image_chw = image_chw\n",
        "        self.network = network.to(device)\n",
        "        self.betas = torch.linspace(min_beta, max_beta, n_steps).to(device)  # Number of steps is typically in the order of thousands\n",
        "        # note : the schedule of betas is an hyperparameter. alphas and alpha_bars are just computed from these betas\n",
        "        self.alphas = 1 - self.betas\n",
        "        self.alpha_bars = torch.tensor([torch.prod(self.alphas[:i + 1]) for i in range(len(self.alphas))]).to(device)\n",
        "\n",
        "    def forward(self, x0, t, eta=None):\n",
        "        # Make input image more noisy (we can directly skip to the desired step)\n",
        "        n, c, h, w = x0.shape\n",
        "        a_bar = self.alpha_bars[t]\n",
        "\n",
        "        if eta is None:\n",
        "            eta = torch.randn(n, c, h, w).to(self.device)\n",
        "\n",
        "        noisy = a_bar.sqrt().reshape(n, 1, 1, 1) * x0 + (1 - a_bar).sqrt().reshape(n, 1, 1, 1) * eta\n",
        "        return noisy\n",
        "\n",
        "    def backward(self, x, t):\n",
        "        # Run each image through the network for each timestep t in the vector t.\n",
        "        # The network returns its estimation of the noise that was added.\n",
        "        return self.network(x, t)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbDZPtutErS8"
      },
      "source": [
        "## Visualizing forward and backward\n",
        "\n",
        "Now that we have defined the high-level functioning of a DDPM model, we can already define some related utility functions.\n",
        "\n",
        "In particular, we will be showing the forward process (which is independent of the denoising network) with the `show_forward` method. \n",
        "\n",
        "We run the backward pass and generate new images with the `generate_new_images` method, but this time we will put more effort into the function and also make it such that a GIF image is created. Notice that in the paper (https://arxiv.org/pdf/2006.11239.pdf) by Ho et. al., two options are considered for $\\sigma_t^2$:\n",
        "\n",
        "- $\\sigma_t^2$ = $\\beta_t$\n",
        "- $\\sigma_t^2$ = $\\frac{1 - \\bar{\\alpha_{t-1}}}{1 - \\bar{\\alpha_{t}}} \\beta_t$\n",
        "\n",
        "In this implementation, they are both a few line-comments away. However, the two terms are rougly always the same and little difference is noticeable. By default, I choose the first option out of simplicity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SY416VfcErbO"
      },
      "outputs": [],
      "source": [
        "def show_forward(ddpm, loader, device):\n",
        "    # Showing the forward process\n",
        "    for batch in loader:\n",
        "        imgs = batch[0]\n",
        "\n",
        "        show_images(imgs, \"Original images\")\n",
        "\n",
        "        for percent in [0.25, 0.5, 0.75, 1]:\n",
        "            show_images(\n",
        "                ddpm(imgs.to(device),\n",
        "                     [int(percent * ddpm.n_steps) - 1 for _ in range(len(imgs))]),\n",
        "                f\"DDPM Noisy images {int(percent * 100)}%\"\n",
        "            )\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQGApj4_Ewmt"
      },
      "outputs": [],
      "source": [
        "def generate_new_images(ddpm, n_samples=16, device=None, frames_per_gif=100, gif_name=\"sampling.gif\", c=1, h=28, w=28):\n",
        "    \"\"\"Given a DDPM model, a number of samples to be generated and a device, returns some newly generated samples\"\"\"\n",
        "    frame_idxs = np.linspace(0, ddpm.n_steps, frames_per_gif).astype(np.uint)\n",
        "    frames = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        if device is None:\n",
        "            device = ddpm.device\n",
        "\n",
        "        # Starting from random noise\n",
        "        x = torch.randn(n_samples, c, h, w).to(device)\n",
        "\n",
        "        for idx, t in enumerate(list(range(ddpm.n_steps))[::-1]):\n",
        "            # Estimating noise to be removed\n",
        "            time_tensor = (torch.ones(n_samples, 1) * t).to(device).long()\n",
        "            eta_theta = ddpm.backward(x, time_tensor)\n",
        "\n",
        "            alpha_t = ddpm.alphas[t]\n",
        "            alpha_t_bar = ddpm.alpha_bars[t]\n",
        "\n",
        "            # Partially denoising the image\n",
        "            x = (1 / alpha_t.sqrt()) * (x - (1 - alpha_t) / (1 - alpha_t_bar).sqrt() * eta_theta)\n",
        "\n",
        "            if t > 0:\n",
        "                z = torch.randn(n_samples, c, h, w).to(device)\n",
        "\n",
        "                # Option 1: sigma_t squared = beta_t\n",
        "                beta_t = ddpm.betas[t]\n",
        "                sigma_t = beta_t.sqrt()\n",
        "\n",
        "                # Option 2: sigma_t squared = beta_tilda_t\n",
        "                # prev_alpha_t_bar = ddpm.alpha_bars[t-1] if t > 0 else ddpm.alphas[0]\n",
        "                # beta_tilda_t = ((1 - prev_alpha_t_bar)/(1 - alpha_t_bar)) * beta_t\n",
        "                # sigma_t = beta_tilda_t.sqrt()\n",
        "\n",
        "                # Adding some more noise like in Langevin Dynamics fashion\n",
        "                x = x + sigma_t * z\n",
        "\n",
        "            # Adding frames to the GIF\n",
        "            if idx in frame_idxs or t == 0:\n",
        "                # Putting digits in range [0, 255]\n",
        "                normalized = x.clone()\n",
        "                for i in range(len(normalized)):\n",
        "                    normalized[i] -= torch.min(normalized[i])\n",
        "                    normalized[i] *= 255 / torch.max(normalized[i])\n",
        "\n",
        "                # Reshaping batch (n, c, h, w) to be a (as much as it gets) square frame\n",
        "                frame = einops.rearrange(normalized, \"(b1 b2) c h w -> (b1 h) (b2 w) c\", b1=int(n_samples ** 0.5))\n",
        "                frame = frame.cpu().numpy().astype(np.uint8)\n",
        "\n",
        "                # Rendering frame\n",
        "                frames.append(frame)\n",
        "\n",
        "    # Storing the gif\n",
        "    with imageio.get_writer(gif_name, mode=\"I\") as writer:\n",
        "        for idx, frame in enumerate(frames):\n",
        "            writer.append_data(frame)\n",
        "            if idx == len(frames) - 1:\n",
        "                for _ in range(frames_per_gif // 3):\n",
        "                    writer.append_data(frames[-1])\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GDfFXcGDe3D"
      },
      "source": [
        "# UNet architecture\n",
        "\n",
        "Okay great! All that concerns DDPM is down on the table already. So now we simply define an architecture that will be responsible of denoising the we should be good to go... Not so fast! While in principle that's true, we have to be careful to conditioning our model with the temporal information. \n",
        "\n",
        "Remember that the only term of the loss function that we really care about is $||\\epsilon - \\epsilon_\\theta(\\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1 - \\bar{\\alpha}_t}\\epsilon, t)||^2$, where $\\epsilon$ is some random noise and $\\epsilon_\\theta$ is the model's prediction of the noise. Now, $\\epsilon_\\theta$ is a function of both $x$ and $t$ and we don't want to have a distinct model for each denoising step (thousands of independent models), but instead we want to use a single model that takes as input the image $x$ and the scalar value indicating the timestep $t$.\n",
        "\n",
        "To do so, in practice we use a sinusoidal embedding (function `sinusoidal_embedding`) that maps each time-step to a `time_emb_dim` dimension. These time embeddings are further mapped with some time-embedding MLPs (function `_make_te`) and added to tensors through the network in a channel-wise manner.\n",
        "\n",
        "**NOTE:** This UNet architecture is purely arbitrary and was desined to work with 28x28 spatial resolution images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-y4vi6nCDfDG"
      },
      "outputs": [],
      "source": [
        "def sinusoidal_embedding(n, d):\n",
        "    # Returns the standard positional embedding\n",
        "    embedding = torch.tensor([[i / 10_000 ** (2 * j / d) for j in range(d)] for i in range(n)])\n",
        "    sin_mask = torch.arange(0, n, 2)\n",
        "\n",
        "    embedding[sin_mask] = torch.sin(embedding[sin_mask])\n",
        "    embedding[1 - sin_mask] = torch.cos(embedding[sin_mask])\n",
        "\n",
        "    return embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WR-K3U6rDiJN"
      },
      "outputs": [],
      "source": [
        "class MyBlock(nn.Module):\n",
        "    def __init__(self, shape, in_c, out_c, kernel_size=3, stride=1, padding=1, activation=None, normalize=True):\n",
        "        super(MyBlock, self).__init__()\n",
        "        self.ln = nn.LayerNorm(shape)\n",
        "        self.conv1 = nn.Conv2d(in_c, out_c, kernel_size, stride, padding)\n",
        "        self.conv2 = nn.Conv2d(out_c, out_c, kernel_size, stride, padding)\n",
        "        self.activation = nn.SiLU() if activation is None else activation\n",
        "        self.normalize = normalize\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.ln(x) if self.normalize else x\n",
        "        out = self.conv1(out)\n",
        "        out = self.activation(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.activation(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqvzCL5eDnSf"
      },
      "outputs": [],
      "source": [
        "class MyUNet(nn.Module):\n",
        "    def __init__(self, n_steps=1000, time_emb_dim=100):\n",
        "        super(MyUNet, self).__init__()\n",
        "\n",
        "        # Sinusoidal embedding\n",
        "        self.time_embed = nn.Embedding(n_steps, time_emb_dim)\n",
        "        self.time_embed.weight.data = sinusoidal_embedding(n_steps, time_emb_dim)\n",
        "        self.time_embed.requires_grad_(False)\n",
        "\n",
        "        # First half\n",
        "        self.te1 = self._make_te(time_emb_dim, 1)\n",
        "        self.b1 = nn.Sequential(\n",
        "            MyBlock((1, 28, 28), 1, 10),\n",
        "            MyBlock((10, 28, 28), 10, 10),\n",
        "            MyBlock((10, 28, 28), 10, 10)\n",
        "        )\n",
        "        self.down1 = nn.Conv2d(10, 10, 4, 2, 1)\n",
        "\n",
        "        self.te2 = self._make_te(time_emb_dim, 10)\n",
        "        self.b2 = nn.Sequential(\n",
        "            MyBlock((10, 14, 14), 10, 20),\n",
        "            MyBlock((20, 14, 14), 20, 20),\n",
        "            MyBlock((20, 14, 14), 20, 20)\n",
        "        )\n",
        "        self.down2 = nn.Conv2d(20, 20, 4, 2, 1)\n",
        "\n",
        "        self.te3 = self._make_te(time_emb_dim, 20)\n",
        "        self.b3 = nn.Sequential(\n",
        "            MyBlock((20, 7, 7), 20, 40),\n",
        "            MyBlock((40, 7, 7), 40, 40),\n",
        "            MyBlock((40, 7, 7), 40, 40)\n",
        "        )\n",
        "        self.down3 = nn.Sequential(\n",
        "            nn.Conv2d(40, 40, 2, 1),\n",
        "            nn.SiLU(),\n",
        "            nn.Conv2d(40, 40, 4, 2, 1)\n",
        "        )\n",
        "\n",
        "        # Bottleneck\n",
        "        self.te_mid = self._make_te(time_emb_dim, 40)\n",
        "        self.b_mid = nn.Sequential(\n",
        "            MyBlock((40, 3, 3), 40, 20),\n",
        "            MyBlock((20, 3, 3), 20, 20),\n",
        "            MyBlock((20, 3, 3), 20, 40)\n",
        "        )\n",
        "\n",
        "        # Second half\n",
        "        self.up1 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(40, 40, 4, 2, 1),\n",
        "            nn.SiLU(),\n",
        "            nn.ConvTranspose2d(40, 40, 2, 1)\n",
        "        )\n",
        "\n",
        "        self.te4 = self._make_te(time_emb_dim, 80)\n",
        "        self.b4 = nn.Sequential(\n",
        "            MyBlock((80, 7, 7), 80, 40),\n",
        "            MyBlock((40, 7, 7), 40, 20),\n",
        "            MyBlock((20, 7, 7), 20, 20)\n",
        "        )\n",
        "\n",
        "        self.up2 = nn.ConvTranspose2d(20, 20, 4, 2, 1)\n",
        "        self.te5 = self._make_te(time_emb_dim, 40)\n",
        "        self.b5 = nn.Sequential(\n",
        "            MyBlock((40, 14, 14), 40, 20),\n",
        "            MyBlock((20, 14, 14), 20, 10),\n",
        "            MyBlock((10, 14, 14), 10, 10)\n",
        "        )\n",
        "\n",
        "        self.up3 = nn.ConvTranspose2d(10, 10, 4, 2, 1)\n",
        "        self.te_out = self._make_te(time_emb_dim, 20)\n",
        "        self.b_out = nn.Sequential(\n",
        "            MyBlock((20, 28, 28), 20, 10),\n",
        "            MyBlock((10, 28, 28), 10, 10),\n",
        "            MyBlock((10, 28, 28), 10, 10, normalize=False)\n",
        "        )\n",
        "\n",
        "        self.conv_out = nn.Conv2d(10, 1, 3, 1, 1)\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        # x is (N, 2, 28, 28) (image with positional embedding stacked on channel dimension)\n",
        "        t = self.time_embed(t)\n",
        "        n = len(x)\n",
        "        out1 = self.b1(x + self.te1(t).reshape(n, -1, 1, 1))  # (N, 10, 28, 28)\n",
        "        out2 = self.b2(self.down1(out1) + self.te2(t).reshape(n, -1, 1, 1))  # (N, 20, 14, 14)\n",
        "        out3 = self.b3(self.down2(out2) + self.te3(t).reshape(n, -1, 1, 1))  # (N, 40, 7, 7)\n",
        "\n",
        "        out_mid = self.b_mid(self.down3(out3) + self.te_mid(t).reshape(n, -1, 1, 1))  # (N, 40, 3, 3)\n",
        "\n",
        "        out4 = torch.cat((out3, self.up1(out_mid)), dim=1)  # (N, 80, 7, 7)\n",
        "        out4 = self.b4(out4 + self.te4(t).reshape(n, -1, 1, 1))  # (N, 20, 7, 7)\n",
        "\n",
        "        out5 = torch.cat((out2, self.up2(out4)), dim=1)  # (N, 40, 14, 14)\n",
        "        out5 = self.b5(out5 + self.te5(t).reshape(n, -1, 1, 1))  # (N, 10, 14, 14)\n",
        "\n",
        "        out = torch.cat((out1, self.up3(out5)), dim=1)  # (N, 20, 28, 28)\n",
        "        out = self.b_out(out + self.te_out(t).reshape(n, -1, 1, 1))  # (N, 1, 28, 28)\n",
        "\n",
        "        out = self.conv_out(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def _make_te(self, dim_in, dim_out):\n",
        "        return nn.Sequential(\n",
        "            nn.Linear(dim_in, dim_out),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(dim_out, dim_out)\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ci0Qtref8bff"
      },
      "source": [
        "# Instantiating the model\n",
        "\n",
        "We are finally done! Now we simply need to instantiate a model, optionally play a bit with it (show forward and backward processes) and write the usual code that defines a training loop for our model. When the model will be done training, we will test it's generative capabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8YotKGi8dYW"
      },
      "outputs": [],
      "source": [
        "# Defining model\n",
        "n_steps, min_beta, max_beta = 1000, 10 ** -4, 0.02  # Originally used by the authors\n",
        "ddpm = MyDDPM(MyUNet(n_steps), n_steps=n_steps, min_beta=min_beta, max_beta=max_beta, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHUEtZlP8is6"
      },
      "source": [
        "# Optional visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1a5kTn5N8p9r"
      },
      "outputs": [],
      "source": [
        "# Optionally, load a pre-trained model that will be further trained\n",
        "# ddpm.load_state_dict(torch.load(store_path, map_location=device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W1pmuQQP8ixp"
      },
      "outputs": [],
      "source": [
        "# Optionally, show the diffusion (forward) process\n",
        "show_forward(ddpm, loader, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PV6tXQy_8uf1"
      },
      "outputs": [],
      "source": [
        "# Optionally, show the denoising (backward) process\n",
        "generated = generate_new_images(ddpm, gif_name=\"before_training.gif\")\n",
        "show_images(generated, \"Images generated before training\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjgbi0iRDvH_"
      },
      "source": [
        "# Training loop\n",
        "\n",
        "The training loop is fairly simple. With each batch of our dataset, we run the forward process on the batch. We use a different timesteps $t$ for each of the `N` images in our `(N, C, H, W)` batch tensor to guarantee more training stability. The added noise is a `(N, C, H, W)` tensor $\\epsilon$.\n",
        "\n",
        "Once we obtained the noisy images, we try to predict $\\epsilon$ out of them with our network. We optimize with a simple Mean-Squared Error (MSE) loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJcKvYEZDvaa"
      },
      "outputs": [],
      "source": [
        "def training_loop(ddpm, loader, n_epochs, optim, device, display=False, store_path=\"ddpm_model.pt\"):\n",
        "    mse = nn.MSELoss()\n",
        "    best_loss = float(\"inf\")\n",
        "    n_steps = ddpm.n_steps\n",
        "\n",
        "    for epoch in tqdm(range(n_epochs), desc=f\"Training progress\", colour=\"#00ff00\"):\n",
        "        epoch_loss = 0.0\n",
        "        for step, batch in enumerate(tqdm(loader, leave=False, desc=f\"Epoch {epoch + 1}/{n_epochs}\", colour=\"#005500\")):\n",
        "            # Loading data\n",
        "            x0 = batch[0].to(device)\n",
        "            n = len(x0)\n",
        "\n",
        "            # Picking some noise for each of the images in the batch, a timestep and the respective alpha_bars\n",
        "            eta = torch.randn_like(x0).to(device)\n",
        "            t = torch.randint(0, n_steps, (n,)).to(device)\n",
        "\n",
        "            # Computing the noisy image based on x0 and the time-step (forward process)\n",
        "            noisy_imgs = ddpm(x0, t, eta)\n",
        "\n",
        "            # Getting model estimation of noise based on the images and the time-step\n",
        "            eta_theta = ddpm.backward(noisy_imgs, t.reshape(n, -1))\n",
        "\n",
        "            # Optimizing the MSE between the noise plugged and the predicted noise\n",
        "            loss = mse(eta_theta, eta)\n",
        "            optim.zero_grad()\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "\n",
        "            epoch_loss += loss.item() * len(x0) / len(loader.dataset)\n",
        "\n",
        "        # Display images generated at this epoch\n",
        "        if display:\n",
        "            show_images(generate_new_images(ddpm, device=device), f\"Images generated at epoch {epoch + 1}\")\n",
        "\n",
        "        log_string = f\"Loss at epoch {epoch + 1}: {epoch_loss:.3f}\"\n",
        "\n",
        "        # Storing the model\n",
        "        if best_loss > epoch_loss:\n",
        "            best_loss = epoch_loss\n",
        "            torch.save(ddpm.state_dict(), store_path)\n",
        "            log_string += \" --> Best model ever (stored)\"\n",
        "\n",
        "        print(log_string)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRTedmnAD24Z"
      },
      "outputs": [],
      "source": [
        "# Training\n",
        "training_loop(ddpm, loader, n_epochs, optim=Adam(ddpm.parameters(), lr), device=device, store_path=store_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kdY0ZUuD50F"
      },
      "source": [
        "# Testing the trained model\n",
        "\n",
        "Time to check how well our model does. We re-store the best performing model according to our training loss and set it to evaluation mode. Finally, we display a batch of generated images and the relative obtained and nice GIF."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6wGnH4g6D58A"
      },
      "outputs": [],
      "source": [
        "# Loading the trained model\n",
        "best_model = MyDDPM(MyUNet(), n_steps=n_steps, device=device)\n",
        "best_model.load_state_dict(torch.load(store_path, map_location=device))\n",
        "best_model.eval()\n",
        "print(\"Model loaded: Generating new images\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7r_67l-DEL7y"
      },
      "outputs": [],
      "source": [
        "generated = generate_new_images(\n",
        "        best_model,\n",
        "        n_samples=100,\n",
        "        device=device,\n",
        "        gif_name=\"fashion.gif\"\n",
        "    )\n",
        "show_images(generated, \"Final result\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMqv53xOJEyF"
      },
      "source": [
        "# Visualizing the diffusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Mnd5RGOyJFKo"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image\n",
        "\n",
        "Image(open('fashion.gif','rb').read())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pa1lv2lnEOkD"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "In this notebook, we implemented a DDPM PyTorch module from scratch. We used a custom UNet-like architecture and the nice sinusoidal positional-embedding technique to condition the denoising process of the network on the particular time-step. We trained the model on the MNIST / Fashion-MNIST dataset and in only 20 epochs (08:47 minutes using a Tesla T4 GPU) we were able to generate new samples for these toy datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbicqEzLjinX"
      },
      "source": [
        "# Further learning!\n",
        "\n",
        "The vanilla DDPM (the one implemented in this notebook) got promptly improved by a couple of papers. Here, I refer the reader to some of them. Finally I would like to acknowledge the resources I personally used to learn more about DDPM and be able to come up with this notebook.\n",
        "\n",
        "## Papers\n",
        "- **Denoising Diffusion Implicit Models** by Song et. al. (https://arxiv.org/abs/2010.02502);\n",
        "- **Improved Denoising Diffusion Probabilistic Models** by Nichol et. al. (https://arxiv.org/abs/2102.09672);\n",
        "- **Hierarchical Text-Conditional Image Generation with CLIP Latents** by Ramesh et. al. (https://arxiv.org/abs/2204.06125);\n",
        "- **Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding** by Saharia et. al. (https://arxiv.org/abs/2205.11487);\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Acknowledgements\n",
        "\n",
        "This notebook was possible thanks also to these amazing people out there on the web that helped me grasp the math and implementation of DDPMs. Make sure you check them out!\n",
        "\n",
        " - <b>Lilian Weng</b>'s [blog](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/): <i>What are Diffusion Models?</i>\n",
        " - <b>abarankab</b>'s [Github repository](https://github.com/abarankab/DDPM)\n",
        " - <b>Jascha Sohl-Dickstein</b>'s [MIT class](https://www.youtube.com/watch?v=XCUlnHP1TNM&ab_channel=AliJahanian)\n",
        " - <b>Niels Rogge</b> and <b>Kashif Rasul</b> [Huggingface's blog](https://huggingface.co/blog/annotated-diffusion): <i>The Annotated Diffusion Model</i>\n",
        " - <b>Outlier</b>'s [Youtube video](https://www.youtube.com/watch?v=HoKDTa5jHvg&ab_channel=Outlier)\n",
        " - <b>AI Epiphany</b>'s [Youtube video](https://www.youtube.com/watch?v=y7J6sSO1k50&t=450s&ab_channel=TheAIEpiphany)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}