{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZkVfFkyzCB0"
      },
      "source": [
        "Artificial Neural Networks\n",
        "================\n",
        "\n",
        "------\n",
        "**Deep Learning for Computer Vision**<br>\n",
        "(c) Research Group CAMMA, University of Strasbourg<br>\n",
        "Website: http://camma.u-strasbg.fr/\n",
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFRB_1RezCB2"
      },
      "source": [
        "### About this notebook\n",
        "\n",
        "- **Objectives**: \n",
        "  - Train and test simple ANNs\n",
        "  - Experiment with underfitting / overfitting\n",
        "  - Perform experiments on Spiral3 and MNIST\n",
        "  \n",
        "\n",
        "- **Instructions**:\n",
        "  - To make the best use of this notebook, read the provided instructions and code, fill in the *#TODO* blocks, and run the code.\n",
        "  - Load MNIST dataset from https://seafile.unistra.fr/f/11b3075bb2df41cf8db2/?dl=1\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGEN3gYAzCB3"
      },
      "source": [
        "### Warm-up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jV3p4PCozCB4"
      },
      "source": [
        "Go to the [Tensorflow Playground](https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.90143&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false&percTrainData_hide=true&discretize_hide=true&noise_hide=true&problem_hide=true)\n",
        "\n",
        "1. Study the complexity of each of the 4 proposed datasets for classification\n",
        "\n",
        "2. Experiment with the network structure and training parameters to classify each dataset using only the point coordinates as input features \n",
        "\n",
        "3. Study underfitting and overfitting behaviors by acting on network capacity and regularization parameter\n",
        "\n",
        "4. If non-linear features are used as input, how much can you simplify the network structure and training process? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGqze9QUzCB5"
      },
      "source": [
        "### Neural Networks for Classification of Spiral3 Toy Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plL3n7ufzCB6"
      },
      "source": [
        "Import libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vVTWPtOBzCB7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import gzip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CntXxFRKzCB7"
      },
      "source": [
        "Create and visualize the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E6vdf_2vzCB7"
      },
      "outputs": [],
      "source": [
        "def toy_spiral3(N=200, K=3, D=2):\n",
        "  np.random.seed(0)\n",
        "  #N: number of points per class\n",
        "  #D: dimensionality\n",
        "  #K: number of classes\n",
        "  X = np.zeros((N*K,D))\n",
        "  y = np.zeros(N*K, dtype='uint8')\n",
        "  for j in range(K):\n",
        "    ix = range(N*j,N*(j+1))\n",
        "    r = np.linspace(0.0,1,N) # radius\n",
        "    t = 5 + np.linspace(j*4,(j+1)*4,N) + np.random.randn(N)*0.3 # theta\n",
        "    X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n",
        "    y[ix] = j\n",
        "  return X,y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jpve-mQpzCB8"
      },
      "outputs": [],
      "source": [
        "def toy_plot(X,y):  \n",
        "  fig = plt.figure()\n",
        "  plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
        "  plt.xlim([-1.5,1.5])\n",
        "  plt.ylim([-1.5,1.5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3sFf8EtrzCB9"
      },
      "outputs": [],
      "source": [
        "np.random.seed(999) #For reproducibility\n",
        "[X,y] = toy_spiral3(200)\n",
        "toy_plot(X,y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ji9pkxVbzCB9"
      },
      "source": [
        "**Code for training and inference of a 2-layer neural network**\n",
        "\n",
        "Identify the difference compared to the Softmax code from the previous notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1xzrAmuRzCB-"
      },
      "outputs": [],
      "source": [
        "class NN2LClassifier:\n",
        "\n",
        "  def __init__(self, num_hidden=100, nclasses=3, ndims=2):\n",
        "    self.nclasses = nclasses\n",
        "    self.ndims = ndims\n",
        "    self.h = num_hidden # size of hidden layer\n",
        "    self.W1 = None\n",
        "    self.b1 = None\n",
        "    self.W2 = None\n",
        "    self.b2 = None\n",
        "\n",
        "  # Train the classifier's parameters\n",
        "  def train(self, X, y, learning_rate=1e-0, reg_weight=1e-3, num_iters=10000, verbose=True):\n",
        "    #Hyperparameters\n",
        "    #learning_rate: gradient descent step size\n",
        "    #reg_weight: regularization   \n",
        "    N = X.shape[0] #number of data points\n",
        "  \n",
        "    # initialize parameters randomly\n",
        "    self.W1 = 0.01 *np.random.randn(self.ndims,self.h) #NOTE: much faster if you remove 0.01 here\n",
        "    self.b1 = np.zeros((1,self.h))\n",
        "    self.W2 = 0.01 *np.random.randn(self.h,self.nclasses)\n",
        "    self.b2 = np.zeros((1,self.nclasses))\n",
        "\n",
        "    #Gradient descent \n",
        "    for i in range(num_iters):\n",
        "  \n",
        "      # evaluate class scores, [N x K]\n",
        "      hidden = np.maximum(0, np.dot(X, self.W1) + self.b1) #ReLU activation\n",
        "      scores = np.dot(hidden, self.W2) + self.b2\n",
        "  \n",
        "      # compute the class probabilities\n",
        "      expo = np.exp(scores)\n",
        "      softm = expo / np.sum(expo, axis=1, keepdims=True) # [N x K]\n",
        "  \n",
        "      # compute the loss: average cross-entropy loss and regularization\n",
        "      logs = -np.log(softm[range(N),y])\n",
        "      data_loss = np.sum(logs)/N\n",
        "      reg_loss = 0.5*reg_weight*np.sum(self.W1*self.W1) + 0.5*reg_weight*np.sum(self.W2*self.W2)\n",
        "      loss = data_loss + reg_loss\n",
        "\n",
        "      # compute the gradient on scores\n",
        "      dscores = softm\n",
        "      dscores[range(N),y] -= 1\n",
        "      dscores /= N\n",
        "  \n",
        "      # backpropate the gradient to the parameters\n",
        "      # first backprop into parameters W2 and b2\n",
        "      dW2 = np.dot(hidden.T, dscores)\n",
        "      db2 = np.sum(dscores, axis=0, keepdims=True)\n",
        "      # next backprop into hidden layer\n",
        "      dhidden = np.dot(dscores, self.W2.T)\n",
        "      # backprop the ReLU non-linearity\n",
        "      dhidden[hidden <= 0] = 0\n",
        "      # finally into W1,b1\n",
        "      dW1 = np.dot(X.T, dhidden)\n",
        "      db1 = np.sum(dhidden, axis=0, keepdims=True)\n",
        "  \n",
        "      # add regularization gradient contribution\n",
        "      dW2 += reg_weight * self.W2\n",
        "      dW1 += reg_weight * self.W1\n",
        "  \n",
        "      # perform a parameter update\n",
        "      self.W1 += -learning_rate * dW1\n",
        "      self.b1 += -learning_rate * db1\n",
        "      self.W2 += -learning_rate * dW2\n",
        "      self.b2 += -learning_rate * db2\n",
        "      \n",
        "      if verbose and (i % 50 == 0):   #Print loss every few steps\n",
        "        print(\"iteration %d: loss=%f ; train_acc:%f\" % (i, loss, self.accuracy(X,y)) )\n",
        "      \n",
        "    return self.accuracy(X,y)\n",
        "\n",
        "  # predict the classes for all input\n",
        "  def predict(self,X):\n",
        "      scores = np.dot(np.maximum(0, np.dot(X, self.W1) + self.b1),self.W2) + self.b2\n",
        "      predicted_classes = np.argmax(scores, axis=1)\n",
        "      return predicted_classes\n",
        "      \n",
        "  # compute accuracy on x \n",
        "  def accuracy(self,X,y):\n",
        "      predicted_classes = self.predict(X)\n",
        "      accuracy = np.mean(predicted_classes == y)\n",
        "      #print 'training accuracy: %.2f' % (accuracy)\n",
        "      return accuracy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXlX7gXIzCB_"
      },
      "source": [
        "Train the 2-layer neural network on the Spiral3 dataset and display the accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-PzjVP-azCB_"
      },
      "outputs": [],
      "source": [
        "classifier = NN2LClassifier(num_hidden=64)\n",
        "training_acc = classifier.train(X,y,num_iters=3000,learning_rate=1,reg_weight=1e-3)\n",
        "print('training accuracy: %.2f' %training_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkTRh5Y_zCB_"
      },
      "source": [
        "How does this accuracy compare to the previous approach on Spiral3 ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_xVk9xvzCCA"
      },
      "source": [
        "Plot accuracy as a function of number of hidden nodes (e.g. 2^i with i in 0...10 while keeping the other parameters fixed):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69ZP0UnGzCCB"
      },
      "outputs": [],
      "source": [
        "#TODO<\n",
        "#TODO>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pitOgfbwzCCB"
      },
      "source": [
        "### Classification on MNIST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwxoFZCLzCCB"
      },
      "source": [
        "Load the MNIST dataset (images of size 28x28):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FjBYJNfgzCCB"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "path = '/content/drive/MyDrive/datasets/' #TO ADAPT IF NEEDED\n",
        "f = gzip.open(path+'mnist.pkl.gz', 'rb')\n",
        "train_set, valid_set, test_set = pickle.load(f,encoding='bytes')\n",
        "f.close()\n",
        "\n",
        "#%% Shuffle the data and define the data variables\n",
        "X_train,y_train = train_set\n",
        "X_test,y_test = test_set\n",
        "\n",
        "inds=np.arange(0,X_train.shape[0])\n",
        "np.random.shuffle(inds)\n",
        "X_train,y_train = X_train[inds],y_train[inds] \n",
        "\n",
        "inds=np.arange(0,X_test.shape[0])\n",
        "np.random.shuffle(inds)\n",
        "X_test,y_test = X_test[inds],y_test[inds] \n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKGWcvbbzCCB"
      },
      "source": [
        "Train the classifier on this dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9oARPYVzCCB"
      },
      "outputs": [],
      "source": [
        "#TODO<\n",
        "#TODO>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlRH-XGOzCCC"
      },
      "source": [
        "Test the classifier on the test set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tNuDbiN-zCCC"
      },
      "outputs": [],
      "source": [
        "#TODO<\n",
        "#TODO>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VunuAqkzCCC"
      },
      "source": [
        "How do these results compare to the previous classification results on MNIST ?"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}