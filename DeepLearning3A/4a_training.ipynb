{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WLMNhNzrvZN"
      },
      "source": [
        "Training CNNs\n",
        "================\n",
        "------\n",
        "**Deep Learning for Computer Vision**<br>\n",
        "(c) Research Group CAMMA, University of Strasbourg<br>\n",
        "Website: http://camma.u-strasbg.fr/\n",
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbLH0TzEea85"
      },
      "source": [
        "In this lab session we will go over the detailed implementation of a training loop for a CNN model.\n",
        "\n",
        "Those *highly repetitive* exercises are meant to train your fundamentals, without the comfort of high-level APIs. \n",
        "\n",
        "You will have to identify overfitting scenarios and adjust your training process accordingly using the methods presented during the lecture.\n",
        "\n",
        "**Instructions**\n",
        "\n",
        "Import RPS dataset (https://seafile.unistra.fr/f/2d58c54203e6435fbf22/?dl=1) in your google drive 'datasets' folder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FT6F_xd_UfVr"
      },
      "source": [
        "# GPU activation\n",
        "\n",
        "Be sure to have cuda enabled from your computer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_51fkJmjea86"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "atijLt93ea87",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch version:  1.13.1+cpu\n",
            "Is GPU available?:  False\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "import torchvision\n",
        "from torchvision.transforms import ToTensor, ToPILImage, Resize\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import importlib as ipl\n",
        "import numpy as np\n",
        "import random\n",
        "import pickle\n",
        "import os\n",
        "import urllib\n",
        "from timeit import default_timer as timer\n",
        "import gc\n",
        "from zipfile import ZipFile\n",
        "\n",
        "# check the PyTorch version; \n",
        "print(\"PyTorch version: \", torch.__version__) \n",
        "# check the GPU support; shold be yes\n",
        "print(\"Is GPU available?: \", torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arPXK1_dUFsO"
      },
      "source": [
        "The next cells will prepare the dataset you will be working on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-QWGxhW1IwOf"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "filepath='rps'\n",
        "with ZipFile('/content/drive/MyDrive/datasets/RPS.zip', 'r') as zf:\n",
        "  zf.extractall('RPS/')\n",
        "  print(\"Files extracted and folder {} created.\".format(filepath))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlISZeotrvZa"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FCC_id6rvZj"
      },
      "source": [
        "We will be packaging the data into the following PyTorch datasets with a batch size of 16 for speed and convenience."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xc3GwTz2rvZk"
      },
      "outputs": [],
      "source": [
        "transform = torchvision.transforms.Compose([\n",
        "    ToTensor(),\n",
        "    Resize(size=(224,224)),\n",
        "])\n",
        "\n",
        "full_dataset = torchvision.datasets.ImageFolder(root=filepath, transform=transform)\n",
        "class_names = full_dataset.classes\n",
        "print(class_names)\n",
        "\n",
        "items = np.random.permutation(len(full_dataset))\n",
        "val_ratio = 0.1\n",
        "test_ratio = 0.1\n",
        "train_items = items[0:int((1.0-val_ratio-test_ratio)*len(full_dataset))]\n",
        "val_items = items[int((1.0-val_ratio-test_ratio)*len(full_dataset)):int((1.0-test_ratio)*len(full_dataset))]\n",
        "test_items = items[int((1.0-test_ratio)*len(full_dataset)):-1]\n",
        "\n",
        "train_dataset = Subset(full_dataset, train_items)\n",
        "val_dataset = Subset(full_dataset, val_items)\n",
        "test_dataset = Subset(full_dataset, test_items)\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "full_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "train_small_indices = np.random.permutation(train_items[0:int(0.05*len(train_items))])\n",
        "train_small_dataset = Subset(full_dataset, train_small_indices)\n",
        "train_small_dataloader = DataLoader(train_small_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "print(len(train_small_dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPJCJ-18rvZe"
      },
      "source": [
        "x, y = dataset[index] \n",
        "\n",
        "Labels (y_...) 0, 1, 2 correspond to paper, rock and scissors respectively.\n",
        "Images (x_...) are (224, 224, 3) numpy arrays."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bA5-HlOirvZf"
      },
      "source": [
        "### TODO 1: Preview 8 images from the training set. Display the type of move (rock, paper or scissors) in the title. Any comments?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FsZCKWO9Opxn"
      },
      "outputs": [],
      "source": [
        "# Get a batch of training data\n",
        "inputs, classes = next(iter(train_dataloader))\n",
        "\n",
        "# Make a grid from batch\n",
        "out = torchvision.utils.make_grid(inputs[0:8])\n",
        "\n",
        "plt.imshow(ToPILImage()(out))\n",
        "plt.title([class_names[x] for x in classes[0:8]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDRmojPfrvZn"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkJFamVUrvZo"
      },
      "source": [
        "The model is provided in the following cell. Pay attention to the syntax: in *__init__* we define the layers with their properties. In *forward* we establish the sequence of function calls that turns inputs (batch of images) into predictions (batch of class probabilities). Each one of the layer objects behaves like a function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nwWolXLarvZo"
      },
      "outputs": [],
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv_1 = nn.Conv2d(in_channels=3, out_channels=8, kernel_size=5)\n",
        "        self.max_pool_1 = nn.MaxPool2d(kernel_size=5)\n",
        "        self.conv_2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3)\n",
        "        self._flatten = nn.Flatten()\n",
        "        self._dense_1 = nn.Linear(in_features=28224, out_features=2048)\n",
        "        self._dense_2 = nn.Linear(in_features=2048, out_features=4096)\n",
        "        self._dense_3 = nn.Linear(in_features=4096, out_features=3)\n",
        "        self._relu = nn.ReLU()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self._relu(self.conv_1(x))\n",
        "        x = self.max_pool_1(x)\n",
        "        x = self._relu(self.conv_2(x))\n",
        "        x = self._flatten(x)\n",
        "        x = nn.ReLU()(self._dense_1(x))\n",
        "        x = nn.ReLU()(self._dense_2(x))\n",
        "        x = self._dense_3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0uCRRpFrvZs"
      },
      "source": [
        "Here we create a CNN model, as well as a save for its parameters from which you'll restart throughout the lab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6sNRjaQarvZs"
      },
      "outputs": [],
      "source": [
        "cnn_0 = CNN()\n",
        "cnn_0 = cnn_0.to('cuda:0')\n",
        "inp = torch.zeros([5,3,224,224]).to('cuda:0') # create \"fake\" input to run cnn_0 a first time, for allocation (starting point)\n",
        "print(inp.type())\n",
        "sample = cnn_0(inp)\n",
        "print(sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Pch9Px1rvZv"
      },
      "source": [
        "Here is an overview of the model's architecture:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ebp6L2kqrvZv"
      },
      "outputs": [],
      "source": [
        "from torchsummary import summary\n",
        "summary(cnn_0, (3, 224, 224))    # provide \"summary\" with model and input data size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cN2RXLmmoLQY"
      },
      "source": [
        "This - *incomplete* -  function will be used to complete one training step. For now it takes batch of images *inputs*, the corresponding batch of labels *labels*, a model *cnn_model* and returns the loss between the model's predictions and the labels.\n",
        "\n",
        "Again, pay attention to the *cnn_model* object. It acts as a **function** here - takes the batch of images and returns the batch of predictions. The first call to this function will also create the tf.Variable objects for the model parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mIkeSffVrvZy"
      },
      "outputs": [],
      "source": [
        "def train_step(inputs, labels, cnn_model):\n",
        "    cnn_model.train()\n",
        "    data = inputs.to('cuda:0')\n",
        "    outp = cnn_model(data)\n",
        "    loss = torch.mean(torch.nn.CrossEntropyLoss()(outp.to('cpu'), labels)).item()\n",
        "    del data\n",
        "    del outp\n",
        "    torch.cuda.empty_cache()\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGOoQ_VBrvZ2"
      },
      "source": [
        "Similarly, the following function computes the accuracy of the model on a batch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zAR5lLTprvZ3"
      },
      "outputs": [],
      "source": [
        "def eval_step(inputs, labels, cnn_model):\n",
        "    cnn_model.eval()\n",
        "    data = inputs.to('cuda:0')\n",
        "    outp = cnn_model(data)\n",
        "    pred = torch.argmax(outp, axis=1)\n",
        "    acc = torch.mean((labels == pred.to('cpu')).float())\n",
        "    del data\n",
        "    del pred\n",
        "    torch.cuda.empty_cache()\n",
        "    return acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vszdthCdrvZ6"
      },
      "source": [
        "### TODO 2: Run the model on the training set for 100 iterations. Report the loss at each iteration. Plot and comment; what is missing?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6uVw3zDrvZ6"
      },
      "outputs": [],
      "source": [
        "losses = []\n",
        "iteration = 0\n",
        "max_iter = 100\n",
        "\n",
        "for epoch in range(10):\n",
        "    for (x_train_in, y_train_in) in train_dataloader:\n",
        "        iteration += 1 \n",
        "        losses.append(train_step(x_train_in, y_train_in, cnn_0))\n",
        "        if iteration >= max_iter:\n",
        "            break\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "ax.plot(np.arange(iteration), losses)\n",
        "ax.set_xlabel(\"iteration\")\n",
        "ax.set_ylabel(\"classification loss\")\n",
        "\n",
        "print(\"accuracy evaluation on last step : {}\".format(eval_step(x_train_in, y_train_in, cnn_0)))\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riS7Bef1rvZ9"
      },
      "source": [
        "\n",
        "### TODO 3: Fix the *train_step* function to incorporate the update:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Ufml5ZTrvZ-"
      },
      "outputs": [],
      "source": [
        "LEARNING_RATE = 0.005\n",
        "optimizer = torch.optim.SGD(cnn_0.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "def train_step(inputs, labels, cnn_model, optim):\n",
        "    cnn_model.train()\n",
        "    data = inputs.to('cuda:0')\n",
        "    #TODO\n",
        "    outp = cnn_model(data)\n",
        "    loss = torch.mean(torch.nn.CrossEntropyLoss()(outp.to('cpu'), labels))\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "    del data\n",
        "    del outp\n",
        "    torch.cuda.empty_cache()\n",
        "    return loss.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68MhnbhWOpxs"
      },
      "outputs": [],
      "source": [
        "torch.save({\n",
        "            'model_state_dict': cnn_0.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            }, 'start')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Tw93F3VrvaA"
      },
      "source": [
        "### TODO 4: Train the model on the training set for 2000 iterations. Every 100th iteration, report the average loss over the 100 previous iterations. Plot it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WOHMO-kZrvaB"
      },
      "outputs": [],
      "source": [
        "start_t = timer()\n",
        "\n",
        "average_training_losses = []\n",
        "training_losses = []\n",
        "iteration = 0\n",
        "nb_pts = 0 \n",
        "max_iter = 2000\n",
        "stop_epoch = False\n",
        "\n",
        "for epoch in range(100): \n",
        "#TODO\n",
        "\n",
        "end_t = timer()\n",
        "print(\"time_elapsed: {}\".format(end_t - start_t))\n",
        "\n",
        "times = list(range(len(average_training_losses)))\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "ax.plot(np.arange(nb_pts), average_training_losses)\n",
        "ax.set_xlabel(\"iteration * 100\")\n",
        "ax.set_ylabel(\"classification loss\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujOlPBTfrvaE"
      },
      "source": [
        "### TODO 5: Evaluate (report the average accuracy) on the training set, then on the test set. Comment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6G3_pzprvaE"
      },
      "outputs": [],
      "source": [
        "train_accs = []\n",
        "#TODO\n",
        "\n",
        "average_train_acc = sum(train_accs) / len(train_accs)\n",
        "print(\"ACCURACY - TRAINING SET: {}\".format(average_train_acc))\n",
        "\n",
        "test_accs = []\n",
        "#TODO\n",
        "\n",
        "average_test_acc = sum(test_accs) / len(test_accs)\n",
        "print(\"ACCURACY - TEST SET: {}\".format(average_test_acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Yc1E0KMrvaH"
      },
      "source": [
        "### TODO 6: Reset the model's weights, then remove 95% of the data from the training set. Repeat the training process from question 4 on this diminished dataset. Plot the loss, evaluate this model on its training set and the test set. Comment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qUmamB0WOpxu"
      },
      "outputs": [],
      "source": [
        "cnn_0 = CNN()\n",
        "optimizer = optimizer = torch.optim.SGD(cnn_0.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "startpoint = torch.load('start')\n",
        "cnn_0.load_state_dict(startpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(startpoint['optimizer_state_dict'])\n",
        "cnn_0 = cnn_0.to('cuda:0')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95bpyxxArvaI"
      },
      "outputs": [],
      "source": [
        "start_t = timer()\n",
        "\n",
        "average_training_losses = []\n",
        "training_losses = []\n",
        "iteration = 0\n",
        "nb_pts = 0 \n",
        "max_iter = 1000\n",
        "stop_epoch = False\n",
        "\n",
        "for epoch in range(1000000): \n",
        "#TODO\n",
        "\n",
        "\n",
        "end_t = timer()\n",
        "print(\"time_elapsed: {}\".format(end_t - start_t))\n",
        "\n",
        "times = list(range(len(average_training_losses)))\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "ax.plot(times, average_training_losses)\n",
        "ax.set_xlabel(\"iteration / 100\")\n",
        "ax.set_ylabel(\"classification loss\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-hdbHbKMrvaM"
      },
      "outputs": [],
      "source": [
        "train_accs = []\n",
        "#TODO\n",
        "\n",
        "average_train_acc = sum(train_accs) / len(train_accs)\n",
        "print(\"ACCURACY - TRAINING SET: {}\".format(average_train_acc))\n",
        "\n",
        "test_accs = []\n",
        "#TODO\n",
        "\n",
        "average_test_acc = sum(test_accs) / len(test_accs)\n",
        "print(\"ACCURACY - TEST SET: {}\".format(average_test_acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KJpDkeyrvaO"
      },
      "source": [
        "### TODO 7: We are going to modify the training loop to incorporate validation; every 100th iteration, evaluate the model on the entire validation dataset and report the average accuracy (do this for 0 and 95% of data removed). Plot the validation accuracy over the course of the training process and interpret."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FrpL8_MhOpxw"
      },
      "outputs": [],
      "source": [
        "cnn_0 = CNN()\n",
        "optimizer = optimizer = torch.optim.SGD(cnn_0.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "startpoint = torch.load('start')\n",
        "cnn_0.load_state_dict(startpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(startpoint['optimizer_state_dict'])\n",
        "cnn_0 = cnn_0.to('cuda:0')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m7AcaMvZrvaP"
      },
      "outputs": [],
      "source": [
        "start_t = timer()\n",
        "\n",
        "average_training_losses = []\n",
        "average_val_accs = []\n",
        "training_losses = []\n",
        "val_accs = []\n",
        "iteration = 0\n",
        "nb_pts = 0 \n",
        "max_iter = 2000\n",
        "stop_epoch = False\n",
        "\n",
        "for epoch in range(100): \n",
        "#TODO\n",
        "        \n",
        "end_t = timer()\n",
        "print(\"time_elapsed: {}\".format(end_t - start_t))\n",
        "\n",
        "times = list(range(len(average_training_losses)))\n",
        "\n",
        "fig = plt.figure()\n",
        "\n",
        "ax = fig.add_subplot(121)\n",
        "ax.plot(np.arange(nb_pts), average_training_losses)\n",
        "ax.set_xlabel(\"iteration * 100\")\n",
        "ax.set_ylabel(\"classification loss\")\n",
        "\n",
        "ax = fig.add_subplot(122)\n",
        "ax.plot(np.arange(len(average_val_accs)), average_val_accs)\n",
        "ax.set_xlabel(\"iteration * 100\")\n",
        "ax.set_ylabel(\"validation accuracy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8RsFnVFJrvaR"
      },
      "outputs": [],
      "source": [
        "train_accs = []\n",
        "#TODO\n",
        "\n",
        "average_train_acc = sum(train_accs) / len(train_accs)\n",
        "print(\"ACCURACY - TRAINING SET: {}\".format(average_train_acc))\n",
        "\n",
        "test_accs = []\n",
        "#TODO\n",
        "\n",
        "average_test_acc = sum(test_accs) / len(test_accs)\n",
        "print(\"ACCURACY - TEST SET: {}\".format(average_test_acc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZqspDk6Opxx"
      },
      "outputs": [],
      "source": [
        "cnn_0 = CNN()\n",
        "optimizer = optimizer = torch.optim.SGD(cnn_0.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "startpoint = torch.load('start')\n",
        "cnn_0.load_state_dict(startpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(startpoint['optimizer_state_dict'])\n",
        "cnn_0 = cnn_0.to('cuda:0')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cj5dgV_UOpxx"
      },
      "outputs": [],
      "source": [
        "start_t = timer()\n",
        "\n",
        "average_training_losses = []\n",
        "average_val_accs = []\n",
        "training_losses = []\n",
        "val_accs = []\n",
        "iteration = 0\n",
        "nb_pts = 0 \n",
        "max_iter = 2000\n",
        "stop_epoch = False\n",
        "\n",
        "for epoch in range(1000000): \n",
        "#TODO\n",
        "        \n",
        "end_t = timer()\n",
        "print(\"time_elapsed: {}\".format(end_t - start_t))\n",
        "\n",
        "times = list(range(len(average_training_losses)))\n",
        "\n",
        "fig = plt.figure()\n",
        "\n",
        "ax = fig.add_subplot(121)\n",
        "ax.plot(np.arange(nb_pts), average_training_losses)\n",
        "ax.set_xlabel(\"iteration * 100\")\n",
        "ax.set_ylabel(\"classification loss\")\n",
        "\n",
        "ax = fig.add_subplot(122)\n",
        "ax.plot(np.arange(len(average_val_accs)), average_val_accs)\n",
        "ax.set_xlabel(\"iteration * 100\")\n",
        "ax.set_ylabel(\"validation accuracy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THD4Xy0KrvaX"
      },
      "outputs": [],
      "source": [
        "train_accs = []\n",
        "#TODO\n",
        "\n",
        "average_train_acc = sum(train_accs) / len(train_accs)\n",
        "print(\"ACCURACY - TRAINING SET: {}\".format(average_train_acc))\n",
        "\n",
        "test_accs = []\n",
        "#TODO\n",
        "\n",
        "average_test_acc = sum(test_accs) / len(test_accs)\n",
        "print(\"ACCURACY - TEST SET: {}\".format(average_test_acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r91gu_7Hrvad"
      },
      "source": [
        "### TODO 8: We will now incorporate data augmentation into the training set. Choose random transformations in order to return the dataset with randomly transformed images. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0k6QvSgjOpxx"
      },
      "outputs": [],
      "source": [
        "cnn_0 = CNN()\n",
        "optimizer = optimizer = torch.optim.SGD(cnn_0.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "startpoint = torch.load('start')\n",
        "cnn_0.load_state_dict(startpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(startpoint['optimizer_state_dict'])\n",
        "cnn_0 = cnn_0.to('cuda:0')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ckDOEphVrvad"
      },
      "outputs": [],
      "source": [
        "#TODO\n",
        "\n",
        "full_dataset = torchvision.datasets.ImageFolder(root='RPS', transform=transform_rand)\n",
        "\n",
        "items = np.random.permutation(len(full_dataset))\n",
        "val_ratio = 0.1\n",
        "test_ratio = 0.1\n",
        "train_items = items[0:int((1.0-val_ratio-test_ratio)*len(full_dataset))]\n",
        "val_items = items[int((1.0-val_ratio-test_ratio)*len(full_dataset)):int((1.0-test_ratio)*len(full_dataset))]\n",
        "test_items = items[int((1.0-test_ratio)*len(full_dataset)):-1]\n",
        "\n",
        "train_dataset = Subset(full_dataset, train_items)\n",
        "val_dataset = Subset(full_dataset, val_items)\n",
        "test_dataset = Subset(full_dataset, test_items)\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "full_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ll_68hyhrvaf"
      },
      "outputs": [],
      "source": [
        "start_t = timer()\n",
        "\n",
        "average_training_losses = []\n",
        "average_val_accs = []\n",
        "training_losses = []\n",
        "val_accs = []\n",
        "iteration = 0\n",
        "nb_pts = 0 \n",
        "max_iter = 2000\n",
        "stop_epoch = False\n",
        "\n",
        "for epoch in range(1000000): \n",
        "#TODO\n",
        "        \n",
        "end_t = timer()\n",
        "print(\"time_elapsed: {}\".format(end_t - start_t))\n",
        "\n",
        "times = list(range(len(average_training_losses)))\n",
        "\n",
        "fig = plt.figure()\n",
        "\n",
        "ax = fig.add_subplot(121)\n",
        "ax.plot(np.arange(nb_pts), average_training_losses)\n",
        "ax.set_xlabel(\"iteration * 100\")\n",
        "ax.set_ylabel(\"classification loss\")\n",
        "\n",
        "ax = fig.add_subplot(122)\n",
        "ax.plot(np.arange(len(average_val_accs)), average_val_accs)\n",
        "ax.set_xlabel(\"iteration * 100\")\n",
        "ax.set_ylabel(\"validation accuracy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iAVpnykbrvah"
      },
      "outputs": [],
      "source": [
        "train_accs = []\n",
        "#TODO\n",
        "average_train_acc = sum(train_accs) / len(train_accs)\n",
        "print(\"ACCURACY - TRAINING SET: {}\".format(average_train_acc))\n",
        "\n",
        "test_accs = []\n",
        "#TODO\n",
        "\n",
        "average_test_acc = sum(test_accs) / len(test_accs)\n",
        "print(\"ACCURACY - TEST SET: {}\".format(average_test_acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gs4I78Q4rvak"
      },
      "source": [
        "### TODO 9: We will attempt to use l2 regularization to alleviate the overfitting issue. Use the \"weight_decay\" in your optimizer. \n",
        "\n",
        "Train on the small training set; plot loss and validation accuracy. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O2e0Zb7IOpx0"
      },
      "outputs": [],
      "source": [
        "cnn_0 = CNN()\n",
        "optimizer = optimizer = torch.optim.SGD(cnn_0.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
        "\n",
        "startpoint = torch.load('start')\n",
        "cnn_0.load_state_dict(startpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(startpoint['optimizer_state_dict'])\n",
        "cnn_0 = cnn_0.to('cuda:0')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0O19QCeprvan"
      },
      "outputs": [],
      "source": [
        "start_t = timer()\n",
        "\n",
        "average_training_losses = []\n",
        "average_val_accs = []\n",
        "training_losses = []\n",
        "val_accs = []\n",
        "iteration = 0\n",
        "nb_pts = 0 \n",
        "max_iter = 2000\n",
        "stop_epoch = False\n",
        "\n",
        "for epoch in range(1000000): \n",
        "#TODO\n",
        "        \n",
        "end_t = timer()\n",
        "print(\"time_elapsed: {}\".format(end_t - start_t))\n",
        "\n",
        "times = list(range(len(average_training_losses)))\n",
        "\n",
        "fig = plt.figure()\n",
        "\n",
        "ax = fig.add_subplot(121)\n",
        "ax.plot(np.arange(nb_pts), average_training_losses)\n",
        "ax.set_xlabel(\"iteration * 100\")\n",
        "ax.set_ylabel(\"classification loss\")\n",
        "\n",
        "ax = fig.add_subplot(122)\n",
        "ax.plot(np.arange(len(average_val_accs)), average_val_accs)\n",
        "ax.set_xlabel(\"iteration * 100\")\n",
        "ax.set_ylabel(\"validation accuracy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NWJ12Q8frvap"
      },
      "outputs": [],
      "source": [
        "train_accs = []\n",
        "#TODO\n",
        "average_train_acc = sum(train_accs) / len(train_accs)\n",
        "print(\"ACCURACY - TRAINING SET: {}\".format(average_train_acc))\n",
        "\n",
        "test_accs = []\n",
        "#TODO\n",
        "\n",
        "average_test_acc = sum(test_accs) / len(test_accs)\n",
        "print(\"ACCURACY - TEST SET: {}\".format(average_test_acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xy_BMirSea-h"
      },
      "source": [
        "### EXTRA QUESTION 1: Change the code for *CNN* by adding dropout, for example between the flatten and dense_1 layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "286plsUwea-i"
      },
      "source": [
        "### EXTRA QUESTION 2: Repeat the previous experiments with different values for the learning rate and batch size."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTxc6R0oOpx1"
      },
      "source": [
        "### EXTRA QUESTION 3: On the small dataset, save the model when validation accurary is at its highest, and check for test accuracy (early stopping). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NN78f1Jpea-j"
      },
      "source": [
        "### Theoretical questions to think about:\n",
        "- What are the advantages and disadvantages of having a large validation set?\n",
        "- Same question for a small one.\n",
        "- Why do we even have a validation set at all? Why not directly use the test set?\n",
        "- Take another look at the model we used. Can you point out any issues with it?\n",
        "- Manufacturing small examples of overfitting was actually quite challenging. To do this, a certain amount of labels (20% of the training set) had to be corrupted. How does that drive the model into overfitting?"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "toc_visible": true
    },
    "file_extension": ".py",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    },
    "mimetype": "text/x-python",
    "name": "python",
    "npconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "version": 3
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
