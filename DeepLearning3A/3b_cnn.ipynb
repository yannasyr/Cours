{"cells":[{"cell_type":"markdown","metadata":{"id":"CBuAT2En-yKi"},"source":["# <center><font color=green> Introduction to CNN</font></center>\n"]},{"cell_type":"markdown","metadata":{"id":"nQSPESID-yKl"},"source":["Deep Convolutional Neural Networks\n","================\n","------\n","(c) Research Group CAMMA, University of Strasbourg<br>\n","Website: http://camma.u-strasbg.fr\n","\n","-----\n","\n","Welcome to the practical section of the 3rd lecture on deep learning. <br>\n","This exercise is divided into 5 parts. Each part is accompanied by a `TODO` exercise.<br>\n","You also have an additional `Bonus` exercise if you finish very quickly.<br>\n","\n","**Instructions**:\n","* Read all the descriptions and code.\n","* Run the pre-completed cells.\n","* Fill in the `TODO`blocks and run the code.\n","* Ensure you are connected to the internet for the notebook image rendering.\n","* Please ask questions if you have any doubts.\n","\n","--------"]},{"cell_type":"markdown","metadata":{"id":"rrK6ZkfjmI_D"},"source":["## Part 1:  Basic Neural Network Layers\n","\n","<img src=\"https://drive.google.com/uc?id=1QT9Lfw_gk6CSQlaVVgaH6uv2hGGMbP0Q\" width=\"500\" style=\"margin-right: 100px\" align=\"left\"/>\n","\n","\n","**Objectives**:\n","\n","* 1) To implement different layers of a neural network\n","* 2) To implement basic neural network operations\n","* 3) To inspect outputs of the neural network operations"]},{"cell_type":"markdown","metadata":{"id":"gWItlnczmI_E"},"source":["**<br>Imports**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WGiHDfLpmI_E"},"outputs":[],"source":["import torch\n","import numpy as np\n","from matplotlib import pyplot as plt\n","import os\n","import pickle\n","import gzip\n","import urllib\n","from zipfile import ZipFile\n","\n","import torchvision.transforms as TF\n","from torchvision.transforms import ToTensor, ToPILImage, Grayscale\n","\n","device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"xQNbwtPrmI_G"},"outputs":[],"source":["# Run to install pydot\n","!pip install pydot-ng\n","!pip install graphviz\n","!pip install torchsummary"]},{"cell_type":"markdown","metadata":{"id":"9MrNb25kmI_G"},"source":["<br>**Check your GPU and PyTorch version**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EggVFNJemI_H"},"outputs":[],"source":["# check the pytorch version\n","#TODO\n","\n","# check the GPU support; should be yes\n","#TODO\n","\n","# check how many GPU(s) are present\n","print(\"How many GPU(s) ?\", torch.cuda.device_count())\n","\n","# get the information on the gpu:0\n","device0 = torch.device('cuda:0')\n","print(\"Infos :\", torch.cuda.get_device_properties(device0))"]},{"cell_type":"markdown","metadata":{"id":"NhJjHS-UmI_H"},"source":["<br>**Prepare image for neural network**\n","- We can directly read an image as usual\n","- To use the image with neural network layers, we have to convert from 3D to 4D tensors\n","- We also need to convert the data type to float\n","- However, to visualize the image, we convert back to integer and 3D tensor."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tq8xcHQMmI_I"},"outputs":[],"source":["# First, prepare the image\n","from google.colab import drive\n","drive.mount('/content/drive')\n","img = plt.imread(\"/content/drive/MyDrive/datasets/sample_images/lenna.jpg\")\n","\n","# convert to a float tensor\n","img = ToTensor()(img)\n","\n","# For Visualization:\n","# convert to 3D: by removing the batch dimension\n","img_init = ToPILImage()(img)\n","\n","# display\n","plt.imshow(img_init)"]},{"cell_type":"markdown","metadata":{"id":"5Zc9b4hMmI_J"},"source":["**<br><br>PyTorch input image format for NN**\n","- Optimized for (mini)batch treatment of images\n","- CNN input tensor should be of the form [# samples, # Channels, Width Height]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VKIOYdCCmI_J"},"outputs":[],"source":["img_input = torch.unsqueeze(img, dim=0)\n","print(\"Initial image tensor shape : {}\".format(img.shape))\n","print(\"Image tensor fed into Conv2d shape : {}\".format(img_input.shape))"]},{"cell_type":"markdown","metadata":{"id":"wSwzSk-ZmI_K"},"source":["**<br><br>Convolution layer**\n","- Let's build a convolutional layer and perform convolution on the input image\n","- required args: size of filter, kernel_size, strides and padding information"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w9vQ0cF2mI_K"},"outputs":[],"source":["# Apply a random Conv2d layer to an input image\n","conv_img = torch.nn.Conv2d(in_channels=3, out_channels=3, kernel_size=(5,5), stride=(3,3))(img_input)\n","\n","# Prepare for display\n","conv_out = ToPILImage()(torch.squeeze(conv_img))\n","\n","# Visulaize\n","fig = plt.figure(figsize=(15,5))\n","fig.add_subplot(1, 2, 1); plt.imshow(img_init)\n","fig.add_subplot(1, 2, 2); plt.imshow(conv_out)"]},{"cell_type":"markdown","metadata":{"id":"6am4_J9BmI_L"},"source":["**<br><br>Pooling layer**\n","- Let's perform average pooling on the input image\n","- required args: pool_size, strides and padding information"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q5vErbBPmI_L"},"outputs":[],"source":["# Max Pooling using 5x5 kernel, strides of 2 with padding\n","maxpooled_img5x5 =  torch.nn.MaxPool2d(kernel_size=(5,5), stride=(2,2))(img_input)\n","\n","# Max Pooling using 16x16 kernel, strides of 2 with padding\n","maxpooled_img16x16 =  torch.nn.MaxPool2d(kernel_size=(16,16), stride=(2,2))(img_input)\n","\n","# Prepare for display\n","out_p5x5   = ToPILImage()(torch.squeeze(maxpooled_img5x5))\n","out_p16x16 = ToPILImage()(torch.squeeze(maxpooled_img16x16))\n","\n","# Visualize\n","fig = plt.figure(figsize=(15,5))\n","fig.add_subplot(1, 2, 1); plt.imshow(out_p5x5)\n","fig.add_subplot(1, 2, 2); plt.imshow(out_p16x16)"]},{"cell_type":"markdown","metadata":{"id":"7eZohPeXmI_L"},"source":["**<br><br>Dense layer**\n","- To use dense layer, you have to flatten the input first\n","- required args: pool_size, strides and padding information"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vjQty3hHmI_M"},"outputs":[],"source":["# Remember to flatten your inputs before dense layer:\n","flat_img = torch.nn.Flatten()(img_input)\n","input_features = flat_img.shape[1]\n","\n","# Adds a densely-connected layer with 100 units to the model and a relu activation function:\n","features = torch.nn.Linear(in_features=input_features, out_features=100, bias=True)(flat_img)\n","\n","\n","print(\"Before flattening: \", img_input.shape)\n","print(\"After flattening: \", flat_img.shape)\n","print(\"After dense: \", features.shape)"]},{"cell_type":"markdown","metadata":{"id":"kWJI0AvfmI_M"},"source":["#### <font color=red> <br><br><u><b>TODO 1:</b></u></font>\n","Solve the following problem:\n","- x1 = Read the image of icube laboratory and prepare it for neural network operation.\n","- x2 = Convolve (x1) with a 5x5 convolution layer to an output filter size of 64\n","- x3 = average pool (x2) using pool size (5x5)\n","- x4 = Convolve (x3) with a 3x3 convolution layer to an output filter size of 3\n","- Display the final output"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-lE0cpdImI_N"},"outputs":[],"source":["# First, prepare the image (path is '/content/drive/MyDrive/datasets/sample_images/icube.jpg')\n","#TODO\n","\n","# convert to a tensor\n","#TODO\n","\n","# convert to 4D tensor: by adding a batch dimension at axis=0\n","#TODO\n","\n","# convert to 3D: by removing the batch dimension\n","#TODO\n","\n","# convert to PIL format\n","#TODO\n","\n","# display\n","#TODO\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D86euPOZmI_N"},"outputs":[],"source":["# Add Conv 3x3 kernel and 16 filters with strides of 1 and padded. Add a relu activation\n","#TODO"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LBZwhEF1mI_N"},"outputs":[],"source":["# Average Pooling using 64x64 kernel, strides of 2 with padding\n","#TODO"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dLlENfYpmI_N"},"outputs":[],"source":["# Add Conv 3x3 kernel and 3 filters with strides of 1 and padded. Add a relu activation\n","#TODO"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sLOdND6AmI_O"},"outputs":[],"source":["# Prepare for display\n","#TODO\n","\n","# Visulaize\n","plt.imshow(out)"]},{"cell_type":"markdown","metadata":{"id":"N1Wd9Q0XmI_O"},"source":["\n","<br><br>\n","\n","## Part 2: Image Filtering\n","\n","- Do you know that convolution is used for image filtering?\n","- To get both low and high pass filters.\n","- With low level PyTorch layers, we can practise edge detection using sobel filter as shown below.\n","- We create the horizontal filter and transpose it to get the vertical filter.\n","\n","![alt text](https://drive.google.com/uc?id=1byjcx3Y0eJEEEOq5m2xUrq-YZiuIe_Ko)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PYpQDYp6mI_P"},"outputs":[],"source":["# Example 1: Create filters\n","\n","# horizontal filter (w_h)\n","w_h = torch.tensor([[-1,0,1],[-2,0,2],[-1,0,1]], dtype=torch.float32)\n","w_h = torch.unsqueeze(torch.unsqueeze(w_h, dim=0), dim=0)\n","print(w_h.shape)\n","\n","# transpose for vertical filter (w_v)\n","w_v = w_h.permute([0,1,3,2])\n","\n","# display\n","print('horizontal filter')\n","print(w_h[0,0,:,:].numpy())\n","\n","print('vertical filter')\n","print(w_v[0,0,:,:].numpy())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cBbiatjqmI_P"},"outputs":[],"source":["# Example 1: Convolution of the image for edge detection\n","\n","# First, read image\n","img = plt.imread(\"/content/drive/MyDrive/datasets/sample_images/lenna.jpg\"))\n","img = ToTensor()(img)\n","\n","# Convert the image to grayscale\n","img_gray = Grayscale()(img)\n","\n","# Expand to have 4-D image tensor (from color image)\n","img_4d = torch.unsqueeze(img_gray, axis=0)\n","\n","# create an horizontal Sobel filter from Conv2d, then apply to img\n","Sobel_h = torch.nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, stride=1, padding=\"same\", bias=False)\n","Sobel_h.weight = torch.nn.Parameter(w_h)\n","conv_h = Sobel_h(img_4d)\n","\n","# create an vertical Sobel filter from Conv2d, then apply to img\n","Sobel_v = torch.nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, stride=1, padding=\"same\", bias=False)\n","Sobel_v.weight = torch.nn.Parameter(w_v)\n","conv_v = Sobel_v(img_4d)\n","\n","# Combine conv_h and conv_w\n","conv_img = torch.sqrt(torch.pow(conv_h,2) + torch.pow(conv_v,2))\n","\n","\n","# Prepare for display\n","img_visual      = ToPILImage()(torch.squeeze(img))\n","conv_h_visual   = ToPILImage()(torch.squeeze(conv_h))\n","conv_v_visual   = ToPILImage()(torch.squeeze(conv_v))\n","conv_img_visual = ToPILImage()(torch.squeeze(conv_img))\n","\n","# Visulaize the output of a convolution  \n","fig = plt.figure(figsize=(20,5))\n","fig.add_subplot(1, 5, 1); plt.imshow(img_visual)\n","fig.add_subplot(1, 5, 2); plt.imshow(conv_h_visual, cmap='gray')\n","fig.add_subplot(1, 5, 3); plt.imshow(conv_v_visual, cmap='gray')\n","fig.add_subplot(1, 5, 4); plt.imshow(conv_img_visual, cmap='gray')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"PNQgdQqFmI_P"},"source":["<br><br><hr>\n","## Part 3:  Building a Convolutional Neural Network (CNN) Model\n","\n","**Objectives**:\n","\n","* 1) To build a convolutional neural network (CNN) model.\n","* 2) To learn how to build sequential model.\n","* 3) To learn how to build functional model.\n","* 4) To automatically compute the model parameters."]},{"cell_type":"markdown","metadata":{"id":"g23tPjgDmI_P"},"source":["<img src=\"https://www.researchgate.net/profile/Pascal-Scalart/publication/317267407/figure/fig13/AS:667887685627913@1536248241501/Figure-Reseau-convolutionnel-LeNet-5-Le-nombre-de-couches-celui-des-cartes-ainsi-que.ppm\" width=\"800\" style=\"margin-right: 100px\" align=\"center\"/>\n","\n","--------------------\n","- Here is an implementation of a (slightly modified) LeNet5 model in PyTorch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VLDdxibDmI_Q"},"outputs":[],"source":["# Model\n","\n","import torch.nn as nn\n","\n","class LeNet(nn.Module):\n","    def __init__(self):\n","        super(LeNet, self).__init__()\n","        self.relu = nn.ReLU()\n","        self.pool = nn.AvgPool2d(kernel_size=(2, 2), stride=(2, 2))\n","        self.conv1 = nn.Conv2d(\n","            in_channels=1,\n","            out_channels=6,\n","            kernel_size=(5, 5),\n","            stride=(1, 1),\n","            padding=(0, 0),\n","        )\n","        self.conv2 = nn.Conv2d(\n","            in_channels=6,\n","            out_channels=16,\n","            kernel_size=(5, 5),\n","            stride=(1, 1),\n","            padding=(0, 0),\n","        )\n","        self.conv3 = nn.Conv2d(\n","            in_channels=16,\n","            out_channels=120,\n","            kernel_size=(5, 5),\n","            stride=(1, 1),\n","            padding=(0, 0),\n","        )\n","        self.linear1 = nn.Linear(120, 84)\n","        self.linear2 = nn.Linear(84, 10)\n","\n","    def forward(self, x):\n","        x = self.relu(self.conv1(x))\n","        x = self.pool(x)\n","        x = self.relu(self.conv2(x))\n","        x = self.pool(x)\n","        x = self.relu(\n","            self.conv3(x)\n","        )  # num_examples x 120 x 1 x 1 --> num_examples x 120\n","        x = x.reshape(x.shape[0], -1)\n","        x = self.relu(self.linear1(x))\n","        x = self.linear2(x)\n","        return x\n","\n","    \n","x_LeNet = torch.randn(64, 1, 32, 32)\n","model_LeNet = LeNet()\n","out_LeNet = model_LeNet(x_LeNet)\n","print(out_LeNet.shape)"]},{"cell_type":"markdown","metadata":{"id":"079jfftS-yLG"},"source":["\n","\n","- Display the model summary</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4BnrDZ25-yLH"},"outputs":[],"source":["from torchsummary import summary\n","model_LeNet.to('cuda:0')   # torchsummary.summary can only display a model infos if model is casted to a cuda device\n","summary(model_LeNet, (1, 32, 32))     # provide \"summary\" with model and input data size"]},{"cell_type":"markdown","metadata":{"id":"qZc5QAgZmI_R"},"source":["<font color=red><br><br><u><b>TODO 2 : </b></u> </font>"]},{"cell_type":"markdown","metadata":{"id":"yz3Rw0jpmI_R"},"source":["<img src=\"https://drive.google.com/uc?id=1WpFCYmn_sHERY58l3nDz8A67EyMd_kD7\" width=\"500\" style=\"margin-right: 100px\" align=\"center\"/>"]},{"cell_type":"markdown","metadata":{"id":"BmZEGZVlmI_R"},"source":["**Let's build a sequential model of the CNN in the diagram above**\n","- It has 3 convolutional layers, 3 max pooling layers and 2 dense layers\n","- The args for each layer is given in the diagram.\n","- Example Conv 7x7, 64, s2 means: convolution with 7x7 kernel_size, 64 filters and strides of 2. \n","- Choose any number you like for the filters marked `A`, `B` and `C'.\n","- The last dense layer has 2 filters for binary classification."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ANLr10qnmI_R"},"outputs":[],"source":["# Define sequential model\n","class CNN(nn.Module):\n","    def __init__(self, nbchannels1=64, nbchannels2=128, nbchannels3=256):\n","        super(CNN, self).__init__()\n","        \n","        self.relu = #TODO\n","        \n","        self.conv1 = #TODO\n","        self.conv2 = #TODO\n","        self.conv3 = #TODO\n","        \n","        self.pool1 = #TODO\n","        self.pool2 = #TODO\n","        self.pool3 = #TODO\n","        \n","        self.linear1 = #TODO\n","        self.linear2 = #TODO\n","\n","    def forward(self, x):\n","        x = #TODO\n","        return x\n","    \n","x_test = torch.randn(64, 3, 96, 96)\n","model = CNN()\n","out = model(x_test)\n","print(out.shape)"]},{"cell_type":"markdown","metadata":{"id":"7ueil0yt-yLR"},"source":["- Display the model summary</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2am0c5xU-yLR"},"outputs":[],"source":["#TODO"]},{"cell_type":"markdown","metadata":{"id":"qQmabkgO-yLW"},"source":["<br><br><hr>\n","## Part 4:  Training and Visualization\n","\n","<img src=\"https://drive.google.com/uc?id=17qNTciBU7N0aezeUVp3DBee72DNdJrVl\" width=\"500\" style=\"margin-right: 100px\" align=\"left\"/>\n","\n","\n","**Objectives**:\n","\n","* 1) To prepare input pipeline for model training\n","* 2) To train a CNN model.\n","* 3) To practice with loss functions, backpropagation and gradient optimization.\n","* 4) Visualize the model training"]},{"cell_type":"markdown","metadata":{"id":"mpXdCGYY-yLY"},"source":["<br>\n","<br> \n","\n","**Dataset input pipeline**\n","- So we will prepare our dataset to train the model we have built in the last exercise.\n","- The data preparation is the same as we have seen in the last practical.\n"," **_So let's have a recap_**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hOcL6yqvmI_T"},"outputs":[],"source":["# The cat & dog dataset (https://seafile.unistra.fr/f/ca6dc3c7823f44b4a769/?dl=1)\n","f = gzip.open('/content/drive/MyDrive/datasets/td_catsdogs2000_u8.pkl.gz', 'rb')\n","cat_dog_dataset = pickle.load(f,encoding='bytes')\n","f.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SHoJN8jymI_T"},"outputs":[],"source":["# generate random number to shuffle the dataset\n","perm = np.random.randint(2000, size=(2000))\n","\n","# shuffle the dataset\n","cat_dog_dataset = (cat_dog_dataset[0][perm], cat_dog_dataset[1][perm])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NcMvV80N-yLa"},"outputs":[],"source":["# split into train, val and test sets as follows:\n","\n","# train set = first 1200 samples\n","X_train, y_train = (cat_dog_dataset[0][0:1200], cat_dog_dataset[1][0:1200])\n","\n","# val set = 400 samples betwen 1200 and 1600\n","X_val, y_val   = (cat_dog_dataset[0][1200:1600], cat_dog_dataset[1][1200:1600])\n","\n","# test set = last 400 samples\n","X_test, y_test  = (cat_dog_dataset[0][1600:], cat_dog_dataset[1][1600:])\n","\n","# print the shape\n","print(\"Train set shape: data={}, label={}\".format(X_train.shape, y_train.shape))\n","print(\"Val set shape: data={}, label={}\".format(X_val.shape, y_val.shape))\n","print(\"Test set shape: data={}, label={}\".format(X_test.shape, y_test.shape))"]},{"cell_type":"markdown","metadata":{"id":"C-hO3_KHmI_U"},"source":["#### <font color=red> <br><br><u><b>TODO</b></u> 3:</font>\n","- Create a class CatsDogs that inherits torch.utils.data.Dataset\n","- Create train_set, val_set & test_set based on previously defined X_train, y_train ... \n","- Create corresponding dataloaders and test it"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"01qPIIwhmI_U"},"outputs":[],"source":["# Create a CatsDogs dataset class\n","from torch.utils.data import Dataset\n","\n","class CatsDogs(Dataset):\n","    def __init__(self, data, labels, transform=ToTensor()):\n","        #TODO\n","    def __len__(self):\n","        #TODO\n","    \n","    def __getitem__(self, idx):\n","        #TODO\n","        #tip : label must be a torch.long (for compatibility with crossentropy loss calculation)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bYj45VahmI_U"},"outputs":[],"source":["# Create train, val & test sets from CatsDogs dataset class\n","#TODO\n","\n","# retrieve sample #185 and display corresponding image with label as title\n","#TODO"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nHZjFhSPmI_V"},"outputs":[],"source":["# Create train & val & test sets dataloaders\n","from torch.utils.data import DataLoader\n","bs = 64\n","\n","train_loader = #TODO\n","val_loader = #TODO\n","test_loader = #TODO"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3Kv8wnX0mI_W"},"outputs":[],"source":["#create an iterable from train_loader for tests \n","dataiter = iter(train_loader)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"76lx2a4umI_W"},"outputs":[],"source":["#display the first two minibatches (consisting of 2 images each)\n","#TODO"]},{"cell_type":"markdown","metadata":{"id":"X3Yc0qsUmI_W"},"source":["<br>\n","<br> \n","\n","**Training**\n","- Here we will setup the training loop"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OmrPsWebmI_W"},"outputs":[],"source":["# create a cnn model instance from CNN and cast it to \"device\" (which is either \"cpu\" or \"cuda:0\")\n","cnn = CNN()\n","cnn.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0m7fAyAfmI_W"},"outputs":[],"source":["#utility function to compute the model accuracy on a dataloader\n","def accuracy(dataloader, model):\n","    #set model in evaluate mode (to avoid random based layers (dropout, ...) to be activated during inference)\n","    model.eval()\n","    #computation of model accuracy on data\n","    nsample = 0 \n","    nmatch = 0 \n","    for data in dataloader : \n","        inputs, labels = data[0].to(device), data[1].to(device)\n","        labels = torch.squeeze(labels.reshape(1,-1))\n","        nsample += labels.shape[0]\n","        # set torch in no_grad mode to avoid interfering with backward process\n","        with torch.no_grad(): \n","            outputs = model(inputs)\n","            outputs = torch.max(outputs, dim=1).indices\n","        match = torch.sum(outputs == labels)\n","        nmatch += match.item()\n","    #reset model in training mode (reactivate random based layers)\n","    model.train()\n","    return float(nmatch) / float(nsample)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FeuZ843XmI_X"},"outputs":[],"source":["# accuracy check on test dataset (random cnn)\n","acc_test_history = accuracy(test_loader, cnn)\n","print(\"Accuracy on test dataset is {}%\".format(acc_test_history * 100))"]},{"cell_type":"markdown","metadata":{"id":"spVtGfKRmI_X"},"source":["<br>\n","- Is this result coherent to you ? "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iSMNhktlmI_X"},"outputs":[],"source":["#define the optimizer\n","import torch.optim as optim\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(cnn.parameters(), lr=1e-4)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WXrPLRJ5mI_X"},"outputs":[],"source":["# training loop\n","loss_train_history = np.ndarray(0,)\n","loss_val_history = np.ndarray(0,)\n","acc_train_history = np.ndarray(0,)\n","acc_val_history = np.ndarray(0,)\n","\n","for epoch in range(75):  # loop over the dataset multiple times\n","\n","    running_loss = 0.0\n","    n=0\n","    for i, data in enumerate(train_loader, 0):\n","        # get the inputs; data is a list of [inputs, labels]\n","        inputs, labels = data[0].to(device), data[1].to(device)\n","        labels = torch.squeeze(labels.reshape(1,-1))\n","\n","        # zero the parameter gradients\n","        optimizer.zero_grad()\n","\n","        # forward + backward + optimize\n","        outputs = cnn(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        \n","        # print statistics\n","        running_loss += loss.item()\n","        if i % 10 == 9:    # print every 10 mini-batches\n","            # compute & print loss on training data\n","            print('[%d, %5d] loss: %.3f' %\n","                  (epoch + 1, i + 1, running_loss / 10.))\n","            loss_train_history = np.append(loss_train_history, running_loss / 10.) # \"classical\" log\n","            running_loss = 0.0\n","            \n","            # compute & print loss on validation data\n","            n=0\n","            running_val_loss = 0.0\n","            for i, data in enumerate(val_loader, 0):\n","                n += 1\n","                inputs, labels = data[0].to(device), data[1].to(device)\n","                labels = torch.squeeze(labels.reshape(1,-1))\n","                with torch.no_grad(): \n","                    outputs = cnn(inputs)\n","                    val_loss = criterion(outputs, labels)\n","                running_val_loss += val_loss.item()\n","            loss_val_history = np.append(loss_val_history, running_val_loss / n)     # \"classical\" log\n","            print('val_loss : %.3f' % (running_val_loss / n))\n","    \n","    #after each epoch computes the accuracy on training and validation data\n","    epoch_train_acc = accuracy(train_loader, cnn)\n","    acc_train_history = np.append(acc_train_history, epoch_train_acc)\n","    epoch_val_acc = accuracy(val_loader, cnn)\n","    acc_val_history = np.append(acc_val_history, epoch_val_acc)\n"," \n","print('Finished Training')"]},{"cell_type":"markdown","metadata":{"id":"OYsMj-j8mI_Y"},"source":["#### <font color=red> <br><br><u><b>TODO</b></u> 4:</font>\n","- Plot the loss (train and validation) over training\n","- Plot accuracy (train and validation) over epochs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lg9m11OcmI_Z"},"outputs":[],"source":["#TODO"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tq5XYUwMmI_Z"},"outputs":[],"source":["# TODO"]},{"cell_type":"markdown","metadata":{"id":"a9zsaxmVmI_Z"},"source":["<br><br><hr>\n","## Part 5:  Inference\n","\n","**Objectives**:\n","\n","* 1) To test the model on unseen data\n","* 2) To evaluate the model performance"]},{"cell_type":"markdown","metadata":{"id":"lgkY3xhDmI_Z"},"source":["<br>\n","\n","**We test our trained model on the test set**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GQFNfUq5mI_a"},"outputs":[],"source":["acc_test_history = accuracy(test_loader, cnn)\n","print(\"Accuracy on test dataset is {}%\".format(acc_test_history * 100))"]},{"cell_type":"markdown","metadata":{"id":"QBcWZHzKmI_a"},"source":["<br>\n","- Is this result coherent to you ? "]},{"cell_type":"markdown","metadata":{"id":"prkosuqD-yL2"},"source":["\n","\n"]},{"cell_type":"markdown","metadata":{"id":"hxbz-xvy-yL3"},"source":["<hr /><hr />\n","\n","### <font color=green> Congratulations!! this is end of second part </font> "]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.11"}},"nbformat":4,"nbformat_minor":0}